package com.rbs.cdna.listener.service.kafka;

import com.fasterxml.jackson.databind.ObjectMapper;
import lombok.extern.slf4j.Slf4j;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.stereotype.Component;
import software.amazon.awssdk.auth.credentials.DefaultCredentialsProvider;
import software.amazon.awssdk.regions.Region;
import software.amazon.awssdk.services.s3.S3Client;
import software.amazon.awssdk.services.s3.model.*;

import javax.annotation.PostConstruct;
import java.io.BufferedReader;
import java.io.InputStreamReader;
import java.util.*;
import java.util.concurrent.ConcurrentHashMap;
import java.util.stream.Collectors;

/**
 * Reads offset files from S3 and caches them in memory.
 */
@Slf4j
@Component
public class S3OffsetStore {

    @Value("${aws.s3.bucket-name}")
    private String bucketName;

    @Value("${aws.s3.offsets-prefix}")
    private String offsetsPrefix;

    private final Map<String, Map<Integer, Long>> offsetsCache = new ConcurrentHashMap<>();

    private S3Client s3Client;
    private ObjectMapper objectMapper = new ObjectMapper();

    @PostConstruct
    public void init() {
        this.s3Client = S3Client.builder()
                .region(Region.of("us-east-1")) // Set as per your region or configure
                .credentialsProvider(DefaultCredentialsProvider.create())
                .build();

        loadOffsetsFromS3();
    }

    public Map<String, Map<Integer, Long>> getOffsetsCache() {
        return offsetsCache;
    }

    private void loadOffsetsFromS3() {
        try {
            ListObjectsV2Request listReq = ListObjectsV2Request.builder()
                    .bucket(bucketName)
                    .prefix(offsetsPrefix.endsWith("/") ? offsetsPrefix : offsetsPrefix + "/")
                    .build();

            ListObjectsV2Response listRes = s3Client.listObjectsV2(listReq);

            List<S3Object> objects = listRes.contents();

            if (objects.isEmpty()) {
                log.warn("No offset files found in bucket {} with prefix {}", bucketName, offsetsPrefix);
                return;
            }

            for (S3Object obj : objects) {
                String key = obj.key(); // e.g., offsets/topic1/0.json
                try {
                    processFile(key);
                } catch (Exception e) {
                    log.error("Error processing offset file: {}", key, e);
                }
            }
        } catch (Exception e) {
            log.error("Failed to load offsets from S3", e);
        }
    }

    private void processFile(String key) throws Exception {
        GetObjectRequest getReq = GetObjectRequest.builder()
                .bucket(bucketName)
                .key(key)
                .build();

        try (BufferedReader reader = new BufferedReader(new InputStreamReader(
                s3Client.getObject(getReq)))) {

            Map<String, Object> jsonMap = objectMapper.readValue(reader, Map.class);

            Long offset = Long.valueOf(jsonMap.get("offset").toString());

            String[] parts = key.split("/");
            if (parts.length < 3) {
                throw new IllegalArgumentException("Invalid offset file path: " + key);
            }
            String topic = parts[1];
            String partitionStr = parts[2].replace(".json", "");
            int partition = Integer.parseInt(partitionStr);

            offsetsCache.computeIfAbsent(topic, k -> new ConcurrentHashMap<>())
                    .put(partition, offset);

            log.info("Loaded offset for topic={} partition={} offset={}", topic, partition, offset);
        }
    }

    public Long readOffset(String topic, int partition) {
        Map<Integer, Long> topicOffsets = offsetsCache.get(topic);
        if (topicOffsets != null) {
            return topicOffsets.get(partition);
        }
        return null;
    }
}
